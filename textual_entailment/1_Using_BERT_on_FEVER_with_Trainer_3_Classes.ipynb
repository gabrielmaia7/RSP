{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the data produced by `0_fever_decode_wikipage_IDs_to_sentences` to train and evaluate a BERT model for textual entailment.\n",
    "\n",
    "It uses the claim, the labels, and the *first sentence only* of the datasets generated in the mentioned notebook.\n",
    "\n",
    "This is a **bad approach**, and we latter choose to follow a much better one.\n",
    "However, I still need to document this in order to understand what I did 1y ago and why.\n",
    "\n",
    "This version trains for 3 classes (Supports, Refutes, and Not Enough Info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "from seaborn import displot, boxplot\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_NAME = 'bert-base-uncased' # We could use bert-large with the new GPU from gravity. Would it perform better?\n",
    "# It probably would be better to check after the whole pipeline is tested. Also it probably should not be uncased, so let's\n",
    "# test that too!\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 2 # On gravity, we were told to use 2 workers. Perhaps because we have 2 GPUs?\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((145449, 3), (9999, 3), (9999, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = read_csv('./data/support_data_v1/train_wikidecoded.csv', encoding='UTF-8')\n",
    "df_dev = read_csv('./data/support_data_v1/shared_task_dev_wikidecoded.csv', encoding='UTF-8')\n",
    "df_test = read_csv('./data/support_data_v1/shared_task_test_wikidecoded.csv', encoding='UTF-8')\n",
    "\n",
    "columns_to_keep = ['claim','first_sentence','label_numeric']\n",
    "df_train = df_train[columns_to_keep]\n",
    "df_dev = df_dev[columns_to_keep]\n",
    "\n",
    "df_test, df_dev = train_test_split(df_dev, test_size=.5)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_dev = df_dev.reset_index(drop=True)\n",
    "df_train.shape, df_test.shape, df_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    80035\n",
       "2    35639\n",
       "1    29775\n",
       "Name: label_numeric, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label_numeric.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, fast=True)\n",
    "# standard Bert tokenizer. don't know why we make it fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_lengths = True\n",
    "\n",
    "if check_lengths:\n",
    "    # This helps see what a good max length would be\n",
    "    def get_tokenized_len(row):\n",
    "        tokens = tokenizer.encode(row['claim'], row['first_sentence'], max_length=512, truncation='only_second')\n",
    "        return len(tokens)\n",
    "\n",
    "    for df in [df_train, df_test, df_dev]:\n",
    "        df['tokenized_len'] = df.apply(get_tokenized_len, axis=1)\n",
    "\n",
    "    tokenized_lens = df_train['tokenized_len'].tolist() +\\\n",
    "        df_test['tokenized_len'].tolist() +\\\n",
    "        df_dev['tokenized_len'].tolist()\n",
    "\n",
    "    boxplot(x=tokenized_lens)\n",
    "    plt.xlim([0, 512]);\n",
    "    plt.xlabel('Token count');\n",
    "\n",
    "    #print(np.max(tokenized_lens))\n",
    "    \n",
    "MAX_LEN = 400 #This means a few cases get cut down. I don't think they're many, but also don't think there\n",
    "# is a big drop in performance if we let them be? I would compare performance with MAX_LEN=425 and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So here we reate dataset and dataloader instances for the FEVER data from the bad approach\n",
    "# torch.utils.data.Dataset and torch.utils.data.DataLoader were used\n",
    "# information about them can be found here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "class FEVERDataset(Dataset):\n",
    "    def __init__(self, claims, sentences, labels, tokenizer, max_len):\n",
    "        self.claims=claims\n",
    "        self.sentences=sentences\n",
    "        self.labels=labels\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_len=max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.claims))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        claim = self.claims[idx]\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            claim,\n",
    "            sentence,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        return {\n",
    "            'claim': claim,\n",
    "            'sentence': sentence,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long).to(DEVICE)\n",
    "        }\n",
    "    \n",
    "def to_data_loader(df, tokenizer, max_len, batch_size, num_workers):\n",
    "    dataset = FEVERDataset(\n",
    "        claims = df.claim,\n",
    "        sentences = df.first_sentence,\n",
    "        labels = df.label_numeric,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return dataset, DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "train_dataset, train_dataloader = to_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE, NUM_WORKERS)\n",
    "test_dataset, test_dataloader = to_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE, NUM_WORKERS)\n",
    "dev_dataset, dev_dataloader = to_data_loader(df_dev, tokenizer, MAX_LEN, BATCH_SIZE, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 10 16:19:03 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 3070    On   | 00000000:5E:00.0 Off |                  N/A |\r\n",
      "| 30%   32C    P2    45W / 220W |   2282MiB /  7982MiB |     67%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 3070    On   | 00000000:AF:00.0 Off |                  N/A |\r\n",
      "| 30%   31C    P2    44W / 220W |   4178MiB /  7982MiB |     20%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     95696      C   ...onda/envs/cuda/bin/python     2279MiB |\r\n",
      "|    1   N/A  N/A     95696      C   ...onda/envs/cuda/bin/python     4175MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/k20036346/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/k20036346/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 145449\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 54546\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53317' max='54546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53317/54546 6:21:18 < 08:47, 2.33 it/s, Epoch 2.93/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.501508</td>\n",
       "      <td>0.803080</td>\n",
       "      <td>0.802941</td>\n",
       "      <td>0.814252</td>\n",
       "      <td>0.804908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>0.386455</td>\n",
       "      <td>0.856586</td>\n",
       "      <td>0.853852</td>\n",
       "      <td>0.874237</td>\n",
       "      <td>0.858769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>0.650821</td>\n",
       "      <td>0.844284</td>\n",
       "      <td>0.838938</td>\n",
       "      <td>0.872901</td>\n",
       "      <td>0.846848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.356100</td>\n",
       "      <td>0.397619</td>\n",
       "      <td>0.873087</td>\n",
       "      <td>0.872698</td>\n",
       "      <td>0.887917</td>\n",
       "      <td>0.874820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.327900</td>\n",
       "      <td>0.548474</td>\n",
       "      <td>0.865487</td>\n",
       "      <td>0.865532</td>\n",
       "      <td>0.889112</td>\n",
       "      <td>0.867252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>0.411730</td>\n",
       "      <td>0.888089</td>\n",
       "      <td>0.888200</td>\n",
       "      <td>0.897131</td>\n",
       "      <td>0.889524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.326300</td>\n",
       "      <td>0.518736</td>\n",
       "      <td>0.872987</td>\n",
       "      <td>0.873991</td>\n",
       "      <td>0.883339</td>\n",
       "      <td>0.874142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.338200</td>\n",
       "      <td>0.390755</td>\n",
       "      <td>0.896590</td>\n",
       "      <td>0.897077</td>\n",
       "      <td>0.902238</td>\n",
       "      <td>0.897763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.342317</td>\n",
       "      <td>0.897490</td>\n",
       "      <td>0.897105</td>\n",
       "      <td>0.903501</td>\n",
       "      <td>0.898847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.429417</td>\n",
       "      <td>0.894589</td>\n",
       "      <td>0.893821</td>\n",
       "      <td>0.906036</td>\n",
       "      <td>0.896218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.260600</td>\n",
       "      <td>0.393651</td>\n",
       "      <td>0.901190</td>\n",
       "      <td>0.901295</td>\n",
       "      <td>0.908104</td>\n",
       "      <td>0.902479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.299900</td>\n",
       "      <td>0.408637</td>\n",
       "      <td>0.888289</td>\n",
       "      <td>0.886280</td>\n",
       "      <td>0.897652</td>\n",
       "      <td>0.890010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.301300</td>\n",
       "      <td>0.329217</td>\n",
       "      <td>0.883088</td>\n",
       "      <td>0.882118</td>\n",
       "      <td>0.899737</td>\n",
       "      <td>0.884889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.290900</td>\n",
       "      <td>0.413802</td>\n",
       "      <td>0.886889</td>\n",
       "      <td>0.887142</td>\n",
       "      <td>0.899757</td>\n",
       "      <td>0.888366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.294800</td>\n",
       "      <td>0.438213</td>\n",
       "      <td>0.898390</td>\n",
       "      <td>0.898393</td>\n",
       "      <td>0.905588</td>\n",
       "      <td>0.899708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>0.414426</td>\n",
       "      <td>0.906791</td>\n",
       "      <td>0.906911</td>\n",
       "      <td>0.912042</td>\n",
       "      <td>0.907971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.312600</td>\n",
       "      <td>0.362285</td>\n",
       "      <td>0.897390</td>\n",
       "      <td>0.897650</td>\n",
       "      <td>0.905850</td>\n",
       "      <td>0.898691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.398818</td>\n",
       "      <td>0.900790</td>\n",
       "      <td>0.900663</td>\n",
       "      <td>0.909454</td>\n",
       "      <td>0.902186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>0.371086</td>\n",
       "      <td>0.899690</td>\n",
       "      <td>0.899707</td>\n",
       "      <td>0.908204</td>\n",
       "      <td>0.901045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.308400</td>\n",
       "      <td>0.331697</td>\n",
       "      <td>0.898590</td>\n",
       "      <td>0.899081</td>\n",
       "      <td>0.906933</td>\n",
       "      <td>0.899799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.392662</td>\n",
       "      <td>0.907791</td>\n",
       "      <td>0.908133</td>\n",
       "      <td>0.911745</td>\n",
       "      <td>0.908846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.438913</td>\n",
       "      <td>0.893389</td>\n",
       "      <td>0.892839</td>\n",
       "      <td>0.904297</td>\n",
       "      <td>0.894925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.305500</td>\n",
       "      <td>0.407313</td>\n",
       "      <td>0.899490</td>\n",
       "      <td>0.899474</td>\n",
       "      <td>0.907529</td>\n",
       "      <td>0.900853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.375385</td>\n",
       "      <td>0.911391</td>\n",
       "      <td>0.911441</td>\n",
       "      <td>0.916426</td>\n",
       "      <td>0.912543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.358033</td>\n",
       "      <td>0.895690</td>\n",
       "      <td>0.896017</td>\n",
       "      <td>0.905866</td>\n",
       "      <td>0.897027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.299400</td>\n",
       "      <td>0.438256</td>\n",
       "      <td>0.889089</td>\n",
       "      <td>0.888947</td>\n",
       "      <td>0.901624</td>\n",
       "      <td>0.890600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.285300</td>\n",
       "      <td>0.449424</td>\n",
       "      <td>0.895890</td>\n",
       "      <td>0.896831</td>\n",
       "      <td>0.902753</td>\n",
       "      <td>0.896879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>0.371575</td>\n",
       "      <td>0.899890</td>\n",
       "      <td>0.900090</td>\n",
       "      <td>0.904793</td>\n",
       "      <td>0.901040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.307100</td>\n",
       "      <td>0.417202</td>\n",
       "      <td>0.897290</td>\n",
       "      <td>0.896911</td>\n",
       "      <td>0.903784</td>\n",
       "      <td>0.898629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.416365</td>\n",
       "      <td>0.895590</td>\n",
       "      <td>0.894802</td>\n",
       "      <td>0.907694</td>\n",
       "      <td>0.897201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.339184</td>\n",
       "      <td>0.905791</td>\n",
       "      <td>0.905629</td>\n",
       "      <td>0.912713</td>\n",
       "      <td>0.907080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.288900</td>\n",
       "      <td>0.372932</td>\n",
       "      <td>0.905091</td>\n",
       "      <td>0.904728</td>\n",
       "      <td>0.909097</td>\n",
       "      <td>0.906312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.341957</td>\n",
       "      <td>0.913891</td>\n",
       "      <td>0.913767</td>\n",
       "      <td>0.917821</td>\n",
       "      <td>0.915014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.361758</td>\n",
       "      <td>0.909591</td>\n",
       "      <td>0.909353</td>\n",
       "      <td>0.914729</td>\n",
       "      <td>0.910823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.340015</td>\n",
       "      <td>0.912991</td>\n",
       "      <td>0.912979</td>\n",
       "      <td>0.916462</td>\n",
       "      <td>0.914057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.276800</td>\n",
       "      <td>0.372319</td>\n",
       "      <td>0.896490</td>\n",
       "      <td>0.895706</td>\n",
       "      <td>0.907619</td>\n",
       "      <td>0.898084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.254800</td>\n",
       "      <td>0.427204</td>\n",
       "      <td>0.898890</td>\n",
       "      <td>0.899607</td>\n",
       "      <td>0.906741</td>\n",
       "      <td>0.899988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.341830</td>\n",
       "      <td>0.911491</td>\n",
       "      <td>0.912076</td>\n",
       "      <td>0.913492</td>\n",
       "      <td>0.912285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.259900</td>\n",
       "      <td>0.452161</td>\n",
       "      <td>0.902490</td>\n",
       "      <td>0.903173</td>\n",
       "      <td>0.909007</td>\n",
       "      <td>0.903556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.240700</td>\n",
       "      <td>0.431674</td>\n",
       "      <td>0.898490</td>\n",
       "      <td>0.898075</td>\n",
       "      <td>0.904882</td>\n",
       "      <td>0.899854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.419604</td>\n",
       "      <td>0.903590</td>\n",
       "      <td>0.903853</td>\n",
       "      <td>0.911876</td>\n",
       "      <td>0.904857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.373894</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909183</td>\n",
       "      <td>0.915954</td>\n",
       "      <td>0.910307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.235900</td>\n",
       "      <td>0.349803</td>\n",
       "      <td>0.913691</td>\n",
       "      <td>0.913686</td>\n",
       "      <td>0.913813</td>\n",
       "      <td>0.914469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.377684</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.898751</td>\n",
       "      <td>0.908196</td>\n",
       "      <td>0.900415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.381907</td>\n",
       "      <td>0.907191</td>\n",
       "      <td>0.907176</td>\n",
       "      <td>0.913752</td>\n",
       "      <td>0.908451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.215900</td>\n",
       "      <td>0.391199</td>\n",
       "      <td>0.914591</td>\n",
       "      <td>0.914796</td>\n",
       "      <td>0.916912</td>\n",
       "      <td>0.915534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.247100</td>\n",
       "      <td>0.388760</td>\n",
       "      <td>0.909891</td>\n",
       "      <td>0.909521</td>\n",
       "      <td>0.915060</td>\n",
       "      <td>0.911136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.412686</td>\n",
       "      <td>0.899590</td>\n",
       "      <td>0.899295</td>\n",
       "      <td>0.909843</td>\n",
       "      <td>0.901045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>0.344618</td>\n",
       "      <td>0.914291</td>\n",
       "      <td>0.914455</td>\n",
       "      <td>0.917056</td>\n",
       "      <td>0.915266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.462556</td>\n",
       "      <td>0.904690</td>\n",
       "      <td>0.905492</td>\n",
       "      <td>0.909316</td>\n",
       "      <td>0.905506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.256200</td>\n",
       "      <td>0.351863</td>\n",
       "      <td>0.907491</td>\n",
       "      <td>0.907432</td>\n",
       "      <td>0.913476</td>\n",
       "      <td>0.908727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>0.411794</td>\n",
       "      <td>0.902790</td>\n",
       "      <td>0.902826</td>\n",
       "      <td>0.912156</td>\n",
       "      <td>0.904147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.348415</td>\n",
       "      <td>0.916592</td>\n",
       "      <td>0.917242</td>\n",
       "      <td>0.919431</td>\n",
       "      <td>0.917408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>0.402416</td>\n",
       "      <td>0.910391</td>\n",
       "      <td>0.910922</td>\n",
       "      <td>0.917790</td>\n",
       "      <td>0.911504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>0.375508</td>\n",
       "      <td>0.912891</td>\n",
       "      <td>0.913159</td>\n",
       "      <td>0.918222</td>\n",
       "      <td>0.913989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.259800</td>\n",
       "      <td>0.309469</td>\n",
       "      <td>0.918592</td>\n",
       "      <td>0.918793</td>\n",
       "      <td>0.918642</td>\n",
       "      <td>0.919244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.240700</td>\n",
       "      <td>0.385650</td>\n",
       "      <td>0.914291</td>\n",
       "      <td>0.914320</td>\n",
       "      <td>0.917565</td>\n",
       "      <td>0.915336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.213500</td>\n",
       "      <td>0.392964</td>\n",
       "      <td>0.915292</td>\n",
       "      <td>0.915364</td>\n",
       "      <td>0.918877</td>\n",
       "      <td>0.916332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.229800</td>\n",
       "      <td>0.312228</td>\n",
       "      <td>0.913491</td>\n",
       "      <td>0.913666</td>\n",
       "      <td>0.919027</td>\n",
       "      <td>0.914612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.231700</td>\n",
       "      <td>0.372495</td>\n",
       "      <td>0.912391</td>\n",
       "      <td>0.912126</td>\n",
       "      <td>0.917319</td>\n",
       "      <td>0.913591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.239100</td>\n",
       "      <td>0.364340</td>\n",
       "      <td>0.911191</td>\n",
       "      <td>0.911433</td>\n",
       "      <td>0.915267</td>\n",
       "      <td>0.912256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.224300</td>\n",
       "      <td>0.331439</td>\n",
       "      <td>0.918792</td>\n",
       "      <td>0.919266</td>\n",
       "      <td>0.919568</td>\n",
       "      <td>0.919451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.416642</td>\n",
       "      <td>0.903590</td>\n",
       "      <td>0.903827</td>\n",
       "      <td>0.912925</td>\n",
       "      <td>0.904891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.239100</td>\n",
       "      <td>0.379436</td>\n",
       "      <td>0.913791</td>\n",
       "      <td>0.913940</td>\n",
       "      <td>0.917681</td>\n",
       "      <td>0.914851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>0.384427</td>\n",
       "      <td>0.912091</td>\n",
       "      <td>0.912352</td>\n",
       "      <td>0.916976</td>\n",
       "      <td>0.913181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.409698</td>\n",
       "      <td>0.903790</td>\n",
       "      <td>0.903092</td>\n",
       "      <td>0.910417</td>\n",
       "      <td>0.905178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.238800</td>\n",
       "      <td>0.319351</td>\n",
       "      <td>0.908491</td>\n",
       "      <td>0.908442</td>\n",
       "      <td>0.916152</td>\n",
       "      <td>0.909784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.236900</td>\n",
       "      <td>0.374981</td>\n",
       "      <td>0.916392</td>\n",
       "      <td>0.916524</td>\n",
       "      <td>0.921247</td>\n",
       "      <td>0.917479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.239600</td>\n",
       "      <td>0.355992</td>\n",
       "      <td>0.916692</td>\n",
       "      <td>0.916578</td>\n",
       "      <td>0.920185</td>\n",
       "      <td>0.917780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.226300</td>\n",
       "      <td>0.336290</td>\n",
       "      <td>0.917992</td>\n",
       "      <td>0.918350</td>\n",
       "      <td>0.922531</td>\n",
       "      <td>0.918992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.219100</td>\n",
       "      <td>0.374708</td>\n",
       "      <td>0.920692</td>\n",
       "      <td>0.920669</td>\n",
       "      <td>0.921830</td>\n",
       "      <td>0.921574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.229800</td>\n",
       "      <td>0.342328</td>\n",
       "      <td>0.921692</td>\n",
       "      <td>0.921932</td>\n",
       "      <td>0.924556</td>\n",
       "      <td>0.922624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.377327</td>\n",
       "      <td>0.920092</td>\n",
       "      <td>0.919925</td>\n",
       "      <td>0.923513</td>\n",
       "      <td>0.921160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>0.359227</td>\n",
       "      <td>0.916092</td>\n",
       "      <td>0.916029</td>\n",
       "      <td>0.921550</td>\n",
       "      <td>0.917267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.378985</td>\n",
       "      <td>0.922992</td>\n",
       "      <td>0.923327</td>\n",
       "      <td>0.926127</td>\n",
       "      <td>0.923904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.175500</td>\n",
       "      <td>0.371556</td>\n",
       "      <td>0.917292</td>\n",
       "      <td>0.917120</td>\n",
       "      <td>0.921913</td>\n",
       "      <td>0.918440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.199700</td>\n",
       "      <td>0.352618</td>\n",
       "      <td>0.919492</td>\n",
       "      <td>0.919613</td>\n",
       "      <td>0.924459</td>\n",
       "      <td>0.920567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.405310</td>\n",
       "      <td>0.914591</td>\n",
       "      <td>0.914312</td>\n",
       "      <td>0.920889</td>\n",
       "      <td>0.915843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.171600</td>\n",
       "      <td>0.374625</td>\n",
       "      <td>0.910891</td>\n",
       "      <td>0.910832</td>\n",
       "      <td>0.919413</td>\n",
       "      <td>0.912215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.351659</td>\n",
       "      <td>0.921192</td>\n",
       "      <td>0.921363</td>\n",
       "      <td>0.925955</td>\n",
       "      <td>0.922243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>0.356518</td>\n",
       "      <td>0.925193</td>\n",
       "      <td>0.925291</td>\n",
       "      <td>0.926627</td>\n",
       "      <td>0.926027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.173700</td>\n",
       "      <td>0.361653</td>\n",
       "      <td>0.919392</td>\n",
       "      <td>0.919306</td>\n",
       "      <td>0.922354</td>\n",
       "      <td>0.920410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.340552</td>\n",
       "      <td>0.916592</td>\n",
       "      <td>0.916758</td>\n",
       "      <td>0.921732</td>\n",
       "      <td>0.917703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.378077</td>\n",
       "      <td>0.912391</td>\n",
       "      <td>0.912304</td>\n",
       "      <td>0.915798</td>\n",
       "      <td>0.913501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.350859</td>\n",
       "      <td>0.921292</td>\n",
       "      <td>0.921312</td>\n",
       "      <td>0.923616</td>\n",
       "      <td>0.922240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.160800</td>\n",
       "      <td>0.374193</td>\n",
       "      <td>0.914591</td>\n",
       "      <td>0.914559</td>\n",
       "      <td>0.920920</td>\n",
       "      <td>0.915814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.178800</td>\n",
       "      <td>0.401996</td>\n",
       "      <td>0.912591</td>\n",
       "      <td>0.912557</td>\n",
       "      <td>0.919710</td>\n",
       "      <td>0.913843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>0.371978</td>\n",
       "      <td>0.918692</td>\n",
       "      <td>0.918891</td>\n",
       "      <td>0.922533</td>\n",
       "      <td>0.919717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.185500</td>\n",
       "      <td>0.332646</td>\n",
       "      <td>0.921492</td>\n",
       "      <td>0.921574</td>\n",
       "      <td>0.924451</td>\n",
       "      <td>0.922472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.189600</td>\n",
       "      <td>0.337569</td>\n",
       "      <td>0.923492</td>\n",
       "      <td>0.923745</td>\n",
       "      <td>0.926803</td>\n",
       "      <td>0.924440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.397764</td>\n",
       "      <td>0.920892</td>\n",
       "      <td>0.920942</td>\n",
       "      <td>0.924878</td>\n",
       "      <td>0.921951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.164300</td>\n",
       "      <td>0.337819</td>\n",
       "      <td>0.920592</td>\n",
       "      <td>0.920636</td>\n",
       "      <td>0.924432</td>\n",
       "      <td>0.921644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.168100</td>\n",
       "      <td>0.339505</td>\n",
       "      <td>0.922392</td>\n",
       "      <td>0.922477</td>\n",
       "      <td>0.925906</td>\n",
       "      <td>0.923402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>0.349271</td>\n",
       "      <td>0.922492</td>\n",
       "      <td>0.922540</td>\n",
       "      <td>0.926479</td>\n",
       "      <td>0.923536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>0.343735</td>\n",
       "      <td>0.923892</td>\n",
       "      <td>0.924013</td>\n",
       "      <td>0.927363</td>\n",
       "      <td>0.924887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.173800</td>\n",
       "      <td>0.308756</td>\n",
       "      <td>0.922692</td>\n",
       "      <td>0.922803</td>\n",
       "      <td>0.926644</td>\n",
       "      <td>0.923725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.329592</td>\n",
       "      <td>0.925393</td>\n",
       "      <td>0.925590</td>\n",
       "      <td>0.928441</td>\n",
       "      <td>0.926330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.322152</td>\n",
       "      <td>0.922992</td>\n",
       "      <td>0.922910</td>\n",
       "      <td>0.926648</td>\n",
       "      <td>0.924045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.354160</td>\n",
       "      <td>0.923192</td>\n",
       "      <td>0.923184</td>\n",
       "      <td>0.926343</td>\n",
       "      <td>0.924201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>0.337827</td>\n",
       "      <td>0.923892</td>\n",
       "      <td>0.923960</td>\n",
       "      <td>0.927845</td>\n",
       "      <td>0.924927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.149900</td>\n",
       "      <td>0.328599</td>\n",
       "      <td>0.926393</td>\n",
       "      <td>0.926557</td>\n",
       "      <td>0.929762</td>\n",
       "      <td>0.927353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.332600</td>\n",
       "      <td>0.926793</td>\n",
       "      <td>0.926946</td>\n",
       "      <td>0.929443</td>\n",
       "      <td>0.927705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.155400</td>\n",
       "      <td>0.330992</td>\n",
       "      <td>0.924892</td>\n",
       "      <td>0.925125</td>\n",
       "      <td>0.928811</td>\n",
       "      <td>0.925871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.151800</td>\n",
       "      <td>0.343225</td>\n",
       "      <td>0.923192</td>\n",
       "      <td>0.923306</td>\n",
       "      <td>0.927419</td>\n",
       "      <td>0.924230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>0.335445</td>\n",
       "      <td>0.923692</td>\n",
       "      <td>0.923785</td>\n",
       "      <td>0.927674</td>\n",
       "      <td>0.924719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.330122</td>\n",
       "      <td>0.923492</td>\n",
       "      <td>0.923614</td>\n",
       "      <td>0.927830</td>\n",
       "      <td>0.924534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-500/pytorch_model.bin\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-1000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-1000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-1000/pytorch_model.bin\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-1500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-1500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-2000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-2000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-1000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-2500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-2500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-1500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-3000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-3000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-2000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-3500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-3500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-2500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-4000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-4000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-3000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-4500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-4500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-3500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-5000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-5000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-4000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-5500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-5500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-4500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-6000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-6000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-5000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-6500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-6500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-6000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-7000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-7000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-6500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-7500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-7500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-7000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-8000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-8000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-5500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-8500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-8500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-7500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-9000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-9000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-8500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-9500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-9500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-9000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-10000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-10000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-9500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-10500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-10500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-8000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-11000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-11000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-10000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-11500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-11500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-11000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-12000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-12000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-10500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-12500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-12500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-11500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-13000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-13000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-12500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-13500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-13500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-13000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-14000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-14000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-13500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-14500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-14500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-14000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-15000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-15000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-14500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-15500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-15500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-15000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-16000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-16000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-15500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-16500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-16500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-12000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-17000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-17000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-16000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-17500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-17500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-17000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-18000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-18000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-17500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-18500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-18500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-18000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-19000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-19000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-18500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-19500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-19500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-19000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-20000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-20000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-19500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-20500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-20500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-20000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-21000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-21000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-20500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-21500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-21500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-21000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-22000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-22000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-21500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-22500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-22500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-22000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-23000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-23000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-16500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-23500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-23500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-22500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-24000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-24000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-23500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-24500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-24500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-24000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-25000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-25000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-24500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-25500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-25500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-25000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-26000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-26000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-25500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-26500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-26500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-23000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-27000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-27000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-26000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-27500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-27500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-27000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-28000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-28000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-26500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-28500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-28500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-27500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-29000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-29000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-28500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-29500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-29500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-29000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-30000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-30000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-29500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-30500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-30500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-30000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-31000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-31000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-28000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-31500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-31500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-30500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-32000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-32000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-31500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-32500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-32500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-32000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-33000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-33000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-33000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-32500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-33500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-33500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-33000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-34000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-34000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-33500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-34500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-34500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-34000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-35000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-35000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-34500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-35500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-35500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-31000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-36000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-36000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-35000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-36500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-36500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-36500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-35500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-37000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-37000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-36500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-37500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-37500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-37500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-36000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-38000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-38000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-37000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-38500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-38500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-38500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-38000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-39000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-39000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-38500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-39500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-39500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-39500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-39000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-40000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-40000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-39500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-40500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-40500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-40500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-37500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-41000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-41000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-41000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-40000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-41500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-41500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-41500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-41000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-42000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-42000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-42000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-41500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-42500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-42500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-42500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-42000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-43000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-43000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-43000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-42500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-43500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-43500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-43500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-43000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-44000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-44000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-44000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-43500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-44500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-44500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-44500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-44000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-45000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-45000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-45000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-44500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-45500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-45500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-45500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-45000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-46000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-46000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-45500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-46500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-46500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-46500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-46000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-47000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-47000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-47000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-46500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-47500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-47500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-47500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-47000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-48000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-48000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-47500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-48500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-48500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-48500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-40500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-49000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-49000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-49000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-48000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-49500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-49500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-49500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-49000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-50000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-50000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-49500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-51000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-51000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-51000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-50000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-51500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-51500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-51500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-50500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-52000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-52000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-52000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-51500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-52500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-52500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-52500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-52000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-53000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-53000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-53000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/checkpoint-52500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./run/1_Using_BERT_on_FEVER_with_Trainer_3_Classes/',\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",     # Evaluation is done at the end of each epoch.\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints. \n",
    "    dataloader_pin_memory=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels = 3).to(DEVICE)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics      # metrics to be computed\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1295' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 15:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.33259961009025574,\n",
       " 'eval_accuracy': 0.9267926792679267,\n",
       " 'eval_f1': 0.9269455882667527,\n",
       " 'eval_precision': 0.9294430387865695,\n",
       " 'eval_recall': 0.927704976442684,\n",
       " 'eval_runtime': 56.0145,\n",
       " 'eval_samples_per_second': 178.507,\n",
       " 'eval_steps_per_second': 1.41,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.34793147444725037,\n",
       " 'test_accuracy': 0.9241924192419242,\n",
       " 'test_f1': 0.9230951471045626,\n",
       " 'test_precision': 0.9274719589553491,\n",
       " 'test_recall': 0.923149206033825,\n",
       " 'test_runtime': 55.2431,\n",
       " 'test_samples_per_second': 181.0,\n",
       " 'test_steps_per_second': 1.43,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset = test_dataset, metric_key_prefix='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 145449\n",
      "  Batch size = 128\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.12888619303703308,\n",
       " 'train_accuracy': 0.9717289221651575,\n",
       " 'train_f1': 0.9680513738977207,\n",
       " 'train_precision': 0.9708277261298428,\n",
       " 'train_recall': 0.9654084634178749,\n",
       " 'train_runtime': 819.2869,\n",
       " 'train_samples_per_second': 177.531,\n",
       " 'train_steps_per_second': 1.388,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset = train_dataset, metric_key_prefix='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in models/BERT_FEVER_v1_model/config.json\n",
      "Model weights saved in models/BERT_FEVER_v1_model/pytorch_model.bin\n",
      "tokenizer config file saved in models/BERT_FEVER_v1_tok/tokenizer_config.json\n",
      "Special tokens file saved in models/BERT_FEVER_v1_tok/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/BERT_FEVER_v1_tok/tokenizer_config.json',\n",
       " 'models/BERT_FEVER_v1_tok/special_tokens_map.json',\n",
       " 'models/BERT_FEVER_v1_tok/vocab.txt',\n",
       " 'models/BERT_FEVER_v1_tok/added_tokens.json')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('models/BERT_FEVER_v1_model')\n",
    "tokenizer.save_pretrained('models/BERT_FEVER_v1_tok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['supports', 'supports', 'refutes', 'not enough info'], tensor([[ 2.7282,  0.4924, -3.5203],\n",
      "        [ 4.1599, -0.6437, -5.0936],\n",
      "        [-0.6981,  4.9072, -3.9848],\n",
      "        [-2.9713, -2.3553,  6.7513]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "classes = ['supports','refutes','not enough info']\n",
    "\n",
    "claims = [\n",
    "    'Barack Obama was part of the 109th United States Congress.',\n",
    "    'France is a member of the European Air Transport Command.',\n",
    "    'The Moon\\'s diocese is the Roman Catholic Diocese of Arizona.',\n",
    "    'Toyota is a member of the Linux Foundation.'\n",
    "]\n",
    "\n",
    "sentences = [\n",
    "    'OBAMA, Barack, a Senator from Illinois and 44th President of the United States;',\n",
    "    'The driving parties were France and Germany, who looked back at a strong bilateral cooperation in the field of air transport.',\n",
    "    'It might sound strange, but in addition to encompassing nine counties and hundreds of cities, the Diocese of Orlando, Florida also has jurisdiction over an otherworldly object: the Moon.',\n",
    "    'Carmakers are using new technologies to deliver on consumer expectations for the same connectivity in their cars as theyve come to expect in their homes and offices.'\n",
    "]\n",
    "\n",
    "def get_predictions(claims, sentences, soft=False):\n",
    "    enc = tokenizer(claims,sentences,max_length=MAX_LEN,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',).to(DEVICE)\n",
    "\n",
    "    # evaluate model:\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=enc['input_ids'], attention_mask=enc['attention_mask'])\n",
    "        predicted_classes = [classes[i] for i in np.argmax(torch.softmax(out.logits,dim=1).tolist(), axis=1)]\n",
    "        return predicted_classes, torch.softmax(out.logits,dim=1).tolist() if soft else out.logits\n",
    "\n",
    "print(get_predictions(claims, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Linux Foundation is a nonprofit organization.\n",
      "Total: SUPPORTS [10.779036521911621, 1.6979856491088867, 7.522977828979492]\n",
      "\n",
      "The Linux Foundation is not a nonprofit organization.\n",
      "Total: REFUTES [1.2677773237228394, 13.65703010559082, 5.075192451477051]\n",
      "\n",
      "Toyota is a member of the Linux Foundation.\n",
      "Total: SUPPORTS [14.690399169921875, 0.13435021042823792, 5.175251007080078]\n",
      "\n",
      "Toyota is not a member of the Linux Foundation.\n",
      "Total: REFUTES [1.0204172134399414, 14.97512149810791, 4.00446081161499]\n",
      "\n",
      "Paris is a city in France.\n",
      "Total: NOT ENOUGH INFO [0.0005974351661279798, 0.0011609744979068637, 19.998241424560547]\n",
      "\n",
      "Toyota is the owner of Denso.\n",
      "Total: NOT ENOUGH INFO [0.8975404500961304, 3.3360962867736816, 15.766363143920898]\n",
      "\n",
      "Jim Zemlin is the executive director of the Linux Foundation.\n",
      "Total: NOT ENOUGH INFO [5.486236572265625, 1.6466472148895264, 12.867116928100586]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "claims = [\n",
    "    'The Linux Foundation is a nonprofit organization.',\n",
    "    'The Linux Foundation is not a nonprofit organization.',\n",
    "    'Toyota is a member of the Linux Foundation.',\n",
    "    'Toyota is not a member of the Linux Foundation.',\n",
    "    'Paris is a city in France.',\n",
    "    'Toyota is the owner of Denso.',\n",
    "    'Jim Zemlin is the executive director of the Linux Foundation.'\n",
    "]\n",
    "# text from https://www.linuxfoundation.org/press-release/2011/07/toyota-joins-linux-foundation/\n",
    "test_text = '''\n",
    "    Toyota looks towards open innovation and collaboration to help transform auto industry\n",
    "\n",
    "    SAN FRANCISCO, July 5, 2011  The Linux Foundation, the nonprofit organization dedicated to accelerating the growth of Linux, today announced that Toyota is its newest member.\n",
    "\n",
    "    A major shift is underway in the automotive industry. Carmakers are using new technologies to deliver on consumer expectations for the same connectivity in their cars as theyve come to expect in their homes and offices. From dashboard computing to In-Vehicle-Infotainment (IVI), automobiles are becoming the latest wireless devices  on wheels.\n",
    "\n",
    "    The Linux operating system is providing a common platform that helps connect the worlds network of devices, including cars. As an open source operating system, it provides automakers and their partners the flexibility they require to bring to market the latest technology features quickly.\n",
    "\n",
    "    Toyota is joining The Linux Foundation as a Gold member to maximize its own investment in Linux while fostering open innovation throughout the automotive ecosystem.\n",
    "\n",
    "    Linux gives us the flexibility and technology maturity we require to evolve our In-Vehicle-Infotainment and communications systems to address the expectations of our customers, said Kenichi Murata, Project General Manager, Electronics Development Div. 1, TOYOTA MOTOR CORPORATION. The Linux Foundation provides us with a neutral forum in which we can collaborate with the worlds leading technology companies on open innovation that accelerates that evolution.\n",
    "\n",
    "    We are very pleased to welcome Toyota to The Linux Foundation. The companys leadership and proven innovation will bring important contributions to the advancement of Linux, said Jim Zemlin, executive director at The Linux Foundation. Toyotas investment in Linux is a testament to the ubiquity of the operating system and its ability to support the latest market requirements.\n",
    "\n",
    "    About The Linux Foundation\n",
    "    The Linux Foundation is a nonprofit consortium dedicated to fostering the growth of Linux. Founded in 2000, the organization sponsors the work of Linux creator Linus Torvalds and promotes, protects and advances the Linux operating system by marshaling the resources of its members and the open source development community. The Linux Foundation provides a neutral forum for collaboration and education by hosting Linux conferences, including LinuxCon, and generating original Linux research and content that advances the understanding of the Linux platform. Its web properties, including Linux.com, reach approximately two million people per month and include important Linux video resources. The organization also provides extensive Linux training opportunities that feature the Linux kernel communitys leading experts as instructors. Follow The Linux Foundation on Twitter.\n",
    "\n",
    "    ###\n",
    "\n",
    "    Trademarks: The Linux Foundation, Linux Standard Base, MeeGo and Yocto Project are trademarks of The Linux Foundation. Linux is a trademark of Linus Torvalds.\n",
    "'''\n",
    "for claim in claims[:]:\n",
    "    print(claim)\n",
    "    sents = [s.text.strip() for s in list(nlp(test_text).sents)]\n",
    "    cls, outputs = get_predictions([claim]*len(sents), sents, soft=False)\n",
    "    #for s,c,o in list(zip(sents,cls,outputs)):\n",
    "    #    print(s)\n",
    "    #    print(c.upper())\n",
    "    #    print(o)\n",
    "    #    print('--------------------------------------\\n')\n",
    "    print('Total:',end=' ')\n",
    "    sum_scores = torch.sum(torch.softmax(outputs,dim=1), dim=0).tolist()\n",
    "    print(classes[np.argmax(sum_scores)].upper(), sum_scores)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model again for sanity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/BERT_FEVER_v1_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file models/BERT_FEVER_v1_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models/BERT_FEVER_v1_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file models/BERT_FEVER_v1_tok/added_tokens.json. We won't load it.\n",
      "Didn't find file models/BERT_FEVER_v1_tok/tokenizer.json. We won't load it.\n",
      "loading file models/BERT_FEVER_v1_tok/vocab.txt\n",
      "loading file None\n",
      "loading file models/BERT_FEVER_v1_tok/special_tokens_map.json\n",
      "loading file models/BERT_FEVER_v1_tok/tokenizer_config.json\n",
      "loading file None\n"
     ]
    }
   ],
   "source": [
    "model2 = BertForSequenceClassification.from_pretrained('models/BERT_FEVER_v1_model')\n",
    "tokenizer2 = BertTokenizer.from_pretrained('models/BERT_FEVER_v1_tok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9999\n",
      "  Batch size = 128\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.34793147444725037,\n",
       " 'test_accuracy': 0.9241924192419242,\n",
       " 'test_f1': 0.9230951471045626,\n",
       " 'test_precision': 0.9274719589553491,\n",
       " 'test_recall': 0.923149206033825,\n",
       " 'test_runtime': 55.1801,\n",
       " 'test_samples_per_second': 181.207,\n",
       " 'test_steps_per_second': 1.432}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics      # metrics to be computed\n",
    ")\n",
    "\n",
    "trainer2.evaluate(eval_dataset = test_dataset, metric_key_prefix='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supports\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1762, -0.6568, -5.1233]], device='cuda:0'), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "classes = ['supports','refutes','not enough info']\n",
    "claim = 'John was born in December.'\n",
    "sentence = 'John had lunch in December of 1992 with his dad.'\n",
    "enc = tokenizer2(claim,sentence,max_length=MAX_LEN,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',).to(DEVICE)\n",
    "\n",
    "# evaluate model:\n",
    "model2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model2(input_ids=enc['input_ids'], attention_mask=enc['attention_mask'])\n",
    "    print(classes[np.argmax(torch.softmax(out.logits,dim=1).tolist(), axis=1)[0]])\n",
    "    print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.eval() is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn off them during model evaluation, and .eval() will do it for you. In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation:\n",
    "\n",
    "```\n",
    "# evaluate model:\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ...\n",
    "    out_data = model(data)\n",
    "    ...\n",
    "```\n",
    "\n",
    "BUT, don't forget to turn back to training mode after eval step:\n",
    "\n",
    "```\n",
    "# training step\n",
    "...\n",
    "model.train()\n",
    "...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
