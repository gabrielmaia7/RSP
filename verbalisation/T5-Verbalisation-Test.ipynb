{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555c25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph2text.finetune import SummarizationModule, Graph2TextModule\n",
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cebf23e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e3a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL='t5-base'\n",
    "DATA_DIR = './graph2text/data/webnlg'\n",
    "OUTPUT_DIR = './graph2text/outputs/port_test'\n",
    "CHECKPOINT = './graph2text/outputs/t5-base_13881/val_avg_bleu=68.1000-step_count=5.ckpt'\n",
    "MAX_LENGTH = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412e0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser()\n",
    "#parser = pl.Trainer.add_argparse_args(parser)\n",
    "#parser = SummarizationModule.add_model_specific_args(parser, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed54939",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#args = parser.parse_args([\n",
    "#    '--data_dir',DATA_DIR,\n",
    "#    '--task','graph2text',\n",
    "#    '--model_name_or_path',MODEL,\n",
    "#    '--eval_batch_size','8',\n",
    "#    '--gpus','1',\n",
    "#    '--output_dir',OUTPUT_DIR,\n",
    "#    '--checkpoint',CHECKPOINT,\n",
    "#    '--max_source_length','384',\n",
    "#    '--max_target_length','384',\n",
    "#    '--val_max_target_length','384',\n",
    "#    '--test_max_target_length','384',\n",
    "#    '--eval_max_gen_length','384',\n",
    "#    '--do_predict',\n",
    "#    '--eval_beams','3'\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea902e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "override_args = {\n",
    "    'data_dir': DATA_DIR,\n",
    "    'output_dir': OUTPUT_DIR,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10824d96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph2Text hparams are: Namespace(logger=True, checkpoint_callback=True, early_stop_callback=False, default_root_dir=None, gradient_clip_val=0, process_position=0, num_nodes=1, num_processes=1, gpus=1, auto_select_gpus=False, log_gpu_memory=None, progress_bar_refresh_rate=1, overfit_batches=0.0, track_grad_norm=-1, check_val_every_n_epoch=1, fast_dev_run=False, accumulate_grad_batches=1, max_epochs=100, min_epochs=1, max_steps=None, min_steps=None, limit_train_batches=1.0, limit_val_batches=1.0, limit_test_batches=1.0, val_check_interval=1.0, log_save_interval=100, row_log_interval=50, distributed_backend=None, sync_batchnorm=False, precision=32, weights_summary='top', weights_save_path=None, num_sanity_val_steps=2, truncated_bptt_steps=None, resume_from_checkpoint=None, profiler=None, benchmark=False, deterministic=False, reload_dataloaders_every_epoch=False, auto_lr_find=False, replace_sampler_ddp=True, terminate_on_nan=False, auto_scale_batch_size=False, prepare_data_per_node=True, amp_backend='native', amp_level='O2', val_percent_check=None, test_percent_check=None, train_percent_check=None, overfit_pct=None, model_name_or_path='t5-base', config_name='', tokenizer_name=None, cache_dir='', encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, learning_rate=3e-05, lr_scheduler='linear', weight_decay=0.0, adam_epsilon=1e-08, warmup_steps=0, num_workers=4, train_batch_size=4, eval_batch_size=4, adafactor=False, output_dir='./graph2text/outputs/port_test', fp16=False, fp16_opt_level='O2', do_train=True, do_predict=True, seed=42, data_dir='./graph2text/data/webnlg', max_source_length=384, max_target_length=384, val_max_target_length=384, test_max_target_length=384, freeze_encoder=False, freeze_embeds=False, sortish_sampler=False, max_tokens_per_batch=None, logger_name='default', n_train=-1, n_val=-1, n_test=-1, task='graph2text', label_smoothing=0.0, src_lang='', tgt_lang='', eval_beams=3, checkpoint=None, val_metric=None, eval_max_gen_length=384, save_top_k=1, early_stopping_patience=15, git_sha='')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We have added 3 tokens\n",
      "parameters Namespace(logger=True, checkpoint_callback=True, early_stop_callback=False, default_root_dir=None, gradient_clip_val=0, process_position=0, num_nodes=1, num_processes=1, gpus=1, auto_select_gpus=False, log_gpu_memory=None, progress_bar_refresh_rate=1, overfit_batches=0.0, track_grad_norm=-1, check_val_every_n_epoch=1, fast_dev_run=False, accumulate_grad_batches=1, max_epochs=100, min_epochs=1, max_steps=None, min_steps=None, limit_train_batches=1.0, limit_val_batches=1.0, limit_test_batches=1.0, val_check_interval=1.0, log_save_interval=100, row_log_interval=50, distributed_backend=None, sync_batchnorm=False, precision=32, weights_summary='top', weights_save_path=None, num_sanity_val_steps=2, truncated_bptt_steps=None, resume_from_checkpoint=None, profiler=None, benchmark=False, deterministic=False, reload_dataloaders_every_epoch=False, auto_lr_find=False, replace_sampler_ddp=True, terminate_on_nan=False, auto_scale_batch_size=False, prepare_data_per_node=True, amp_backend='native', amp_level='O2', val_percent_check=None, test_percent_check=None, train_percent_check=None, overfit_pct=None, model_name_or_path='t5-base', config_name='', tokenizer_name=None, cache_dir='', encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, learning_rate=3e-05, lr_scheduler='linear', weight_decay=0.0, adam_epsilon=1e-08, warmup_steps=0, num_workers=4, train_batch_size=4, eval_batch_size=4, adafactor=False, output_dir='./graph2text/outputs/port_test', fp16=False, fp16_opt_level='O2', do_train=True, do_predict=True, seed=42, data_dir='./graph2text/data/webnlg', max_source_length=384, max_target_length=384, val_max_target_length=384, test_max_target_length=384, freeze_encoder=False, freeze_embeds=False, sortish_sampler=False, max_tokens_per_batch=None, logger_name='default', n_train=-1, n_val=-1, n_test=-1, task='graph2text', label_smoothing=0.0, src_lang='', tgt_lang='', eval_beams=3, checkpoint=None, val_metric=None, eval_max_gen_length=384, save_top_k=1, early_stopping_patience=15, git_sha='')\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:212: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['model.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "model_ckp = Graph2TextModule.load_from_checkpoint(CHECKPOINT, strict=False, **override_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921439d3",
   "metadata": {},
   "source": [
    "DEALING WITH UNKNOWN TOKENS\n",
    "1. See what words have characters outside the vocab\n",
    "2. replace these chars with </unk> (??)\n",
    "3. create a mapping (like Taej ?? n to Taejŏn)\n",
    "4. Map them back together in the sentence (if it has Taej ?? n, replace with Taejŏn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c8879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ckp.tokenizer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14790d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13959,     3, 21094,    12,  1566,    10, 32100,  1150,  6550,  1166,\n",
      "         32101,  3902, 32102,  2382,     3,  4401, 32100,  1150,  6550,  1166,\n",
      "         32101,    19,     3,     9, 32102,  7293,     1]])\n",
      "['translate Graph to English:<H> World Trade Center<R> height<T> 200 meter<H> World Trade Center<R> is a<T> tower</s>']\n",
      "['The World Trade Center is a tower with a height of 200 meters.']\n",
      "1.7831895351409912\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "inputs = [\n",
    "    'translate Graph to English: <H> World Trade Center <R> height <T> 200 meter <H> World Trade Center <R> is a <T> tower',\n",
    "]\n",
    "\n",
    "inputs_encoding = model_ckp.tokenizer(\n",
    "    inputs, truncation=True, max_length=MAX_LENGTH, return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(inputs_encoding['input_ids'])\n",
    "\n",
    "now = time.time()\n",
    "\n",
    "model_ckp.model.eval()\n",
    "with torch.no_grad():\n",
    "    gen_output = model_ckp.model.generate(\n",
    "        inputs_encoding['input_ids'],\n",
    "        attention_mask=inputs_encoding['attention_mask'],\n",
    "        use_cache=True,\n",
    "        decoder_start_token_id = model_ckp.decoder_start_token_id,\n",
    "        num_beams=model_ckp.eval_beams,\n",
    "        max_length=model_ckp.eval_max_length,\n",
    "        length_penalty=1.0    \n",
    "    )\n",
    "\n",
    "print([model_ckp.tokenizer.decode(i) for i in inputs_encoding['input_ids']])\n",
    "print([model_ckp.tokenizer.decode(i, skip_special_tokens=True) for i in gen_output])\n",
    "print(time.time() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec123907",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The directions of Khachkar are Գողգոթավանքից.', [{}, {'⁇': 'Գողգոթավանքից'}])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "vocab = model_ckp.tokenizer.get_vocab()\n",
    "convert_some_japanese_characters = True\n",
    "N = 2\n",
    "class UnknownCharReplacer():\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = tokenizer.get_vocab()\n",
    "        self.unknowns = []\n",
    "        \n",
    "    def read_label(self, label):\n",
    "        self.unknowns.append({})\n",
    "        \n",
    "        # Some pre-processing of labels to normalise some characters\n",
    "        if convert_some_japanese_characters:\n",
    "            label = label.replace('（','(')\n",
    "            label = label.replace('）',')')\n",
    "            label = label.replace('〈','<')\n",
    "            label = label.replace('／','/')\n",
    "            label = label.replace('〉','>')\n",
    "        \n",
    "        \n",
    "        label_encoded = self.tokenizer.encode(label)\n",
    "        label_tokens = self.tokenizer.convert_ids_to_tokens(label_encoded)\n",
    "        label_token_to_string = self.tokenizer.convert_tokens_to_string(label_tokens)\n",
    "        unk_token_to_string = model_ckp.tokenizer.convert_tokens_to_string([model_ckp.tokenizer.unk_token])\n",
    "                \n",
    "        #print(label_encoded,label_tokens,label_token_to_string)\n",
    "        \n",
    "        match_unks_in_label = re.findall('(?:(?: )*⁇(?: )*)+', label_token_to_string)\n",
    "        if len(match_unks_in_label) > 0:\n",
    "            # If the whole label is made of UNK\n",
    "            if match_unks_in_label[0] == label_token_to_string:\n",
    "                #print('Label is all unks')\n",
    "                self.unknowns[-1][label_token_to_string.strip()] = label\n",
    "            # Else, there should be non-UNK characters in the label\n",
    "            else:\n",
    "                #print('Label is NOT all unks')\n",
    "                # Analyse the label with a sliding window of size N (N before, N ahead)\n",
    "                for idx, token in enumerate(label_tokens):\n",
    "                    idx_before = max(0,idx-N)\n",
    "                    idx_ahead = min(len(label_tokens), idx+N+1)\n",
    "                    \n",
    "                                       \n",
    "                    # Found a UNK\n",
    "                    if token == self.tokenizer.unk_token:\n",
    "                        \n",
    "                        # In case multiple UNK, exclude UNKs seen after this one, expand window to other side if possible\n",
    "                        if len(match_unks_in_label) > 1:\n",
    "                            #print(idx)\n",
    "                            #print(label_tokens)\n",
    "                            #print(label_tokens[idx_before:idx_ahead])\n",
    "                            #print('HERE!')\n",
    "                            # Reduce on the right, expanding on the left\n",
    "                            while model_ckp.tokenizer.unk_token in label_tokens[idx+1:idx_ahead]:\n",
    "                                idx_before = max(0,idx_before-1)\n",
    "                                idx_ahead = min(idx+2, idx_ahead-1)\n",
    "                                #print(label_tokens[idx_before:idx_ahead])\n",
    "                            # Now just reduce on the left\n",
    "                            while model_ckp.tokenizer.unk_token in label_tokens[idx_before:idx]:\n",
    "                                idx_before = min(idx-1,idx_before+2)\n",
    "                                #print(label_tokens[idx_before:idx_ahead])\n",
    "                                \n",
    "                        # First token of the label is UNK\n",
    "                        span = self.tokenizer.convert_tokens_to_string(label_tokens[idx_before:idx_ahead])\n",
    "                        if idx == 1 and label_tokens[0] == '▁':\n",
    "                            #print('Label begins with unks')\n",
    "                            to_replace = '^' + re.escape(span).replace(\n",
    "                                    re.escape(unk_token_to_string),\n",
    "                                    '.+?'\n",
    "                                )\n",
    "                            \n",
    "                            replaced_span = re.search(\n",
    "                                to_replace,\n",
    "                                label\n",
    "                            )[0]\n",
    "                            self.unknowns[-1][span.strip()] = replaced_span\n",
    "                        # Last token of the label is UNK\n",
    "                        elif idx == len(label_tokens)-2 and label_tokens[-1] == model_ckp.tokenizer.eos_token:\n",
    "                            #print('Label ends with unks')\n",
    "                            pre_idx = self.tokenizer.convert_tokens_to_string(label_tokens[idx_before:idx])\n",
    "                            pre_idx_unk_counts = pre_idx.count(unk_token_to_string)\n",
    "                            to_replace = re.escape(span).replace(\n",
    "                                    re.escape(unk_token_to_string),\n",
    "                                    f'[^{re.escape(pre_idx)}]+?'\n",
    "                                ) + '$'\n",
    "                            \n",
    "                            if pre_idx.strip() == '':\n",
    "                                to_replace = to_replace.replace('[^]', '(?<=\\s)[^a-zA-Z0-9]')\n",
    "                            \n",
    "                            replaced_span = re.search(\n",
    "                                to_replace,\n",
    "                                label\n",
    "                            )[0]\n",
    "                            self.unknowns[-1][span.strip()] = replaced_span\n",
    "                            \n",
    "                        # A token in-between the label is UNK                            \n",
    "                        else:\n",
    "                            #print('Label has unks in the middle')\n",
    "                            pre_idx = self.tokenizer.convert_tokens_to_string(label_tokens[idx_before:idx])\n",
    "\n",
    "                            to_replace = re.escape(span).replace(\n",
    "                                re.escape(unk_token_to_string),\n",
    "                                f'[^{re.escape(pre_idx)}]+?'\n",
    "                            )\n",
    "                            #If there is nothing behind the ??, because it is in the middle but the previous token is also\n",
    "                            #a ??, then we would end up with to_replace beginning with [^], which we can't have\n",
    "                            if pre_idx.strip() == '':\n",
    "                                to_replace = to_replace.replace('[^]', '(?<=\\s)[^a-zA-Z0-9]')\n",
    "        \n",
    "                            replaced_span = re.search(\n",
    "                                to_replace,\n",
    "                                label\n",
    "                            )\n",
    "                            \n",
    "                            if replaced_span:\n",
    "                                span = re.sub(r'\\s([?.!\",](?:\\s|$))', r'\\1', span.strip())\n",
    "                                self.unknowns[-1][span] = replaced_span[0]\n",
    "\n",
    "                    \n",
    "        \n",
    "    def replace_on_sentence(self, sentence):\n",
    "        # Loop through in case the labels are repeated, maximum of three times\n",
    "        loop_n = 3\n",
    "        while '⁇' in sentence and loop_n > 0:\n",
    "            loop_n -= 1\n",
    "            for unknowns in self.unknowns:\n",
    "                for k,v in unknowns.items():\n",
    "                    # In case it is because the first letter of the sentence has been uppercased\n",
    "                    if not k in sentence and k[0] == k[0].lower() and k[0].upper() == sentence[0]:\n",
    "                        k = k[0].upper() + k[1:]\n",
    "                        v = v[0].upper() + v[1:]\n",
    "                    # In case it is because a double space is found where it should not be\n",
    "                    elif not k in sentence and len(re.findall(r'\\s{2,}',k))>0:\n",
    "                        k = re.sub(r'\\s+', ' ', k)\n",
    "                    #print(k,'/',v,'/',sentence)\n",
    "                    sentence = sentence.replace(k.strip(),v.strip(),1)\n",
    "                    #sentence = re.sub(k, v, sentence)\n",
    "            sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "            sentence = re.sub(r'\\s([?.!\",](?:\\s|$))', r'\\1', sentence)\n",
    "        return sentence\n",
    "    \n",
    "replacer = UnknownCharReplacer(model_ckp.tokenizer)\n",
    "replacer.read_label('Cuhppulčohkka')\n",
    "replacer.read_label('Cuhppulčohkka')\n",
    "replacer.replace_on_sentence('Cuhppul ⁇ ohkka is a native label.'), replacer.unknowns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb6185",
   "metadata": {},
   "source": [
    "# STRATEGY TO REPLACE ??\n",
    "\n",
    "- TREAT CASE WHERE THE WHOLE LABEL IS MADE OF UNKNOWNS\n",
    "\n",
    "- IF IT ISNT, THERE MUST BE SPACE BEFORE AND AFTER THAT IS KNOWN\n",
    "     - GROUP UNKNOWNS INTO CONTINUOUS UNK TOKENS (?? ?? -> ??)\n",
    "    \n",
    "\n",
    "    - LOOK TO SEE IF IT IS IN THE BEGINNING OF THE SENTENCE\n",
    "        - TREAT CASE\n",
    "    - LOOK TO SEE IF IT IS IN THE ENDING OF THE SENTENCE\n",
    "        - TREAT CASE\n",
    "    - IT IS IN THE MIDDLE, TREAT AS NORMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "129701e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('sampled_df_pre_verbalisation.csv')\n",
    "df_sample = df.sample(64, random_state=SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca1b705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TripleLabelDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.len = self.df.shape[0]\n",
    "    def __getitem__(self, index):      \n",
    "        row = self.df.iloc[index]\n",
    "        item = f\"translate Graph to English: <H> {row['entity_label']} <R> {row['property_label']} <T> {row['object_label']}\"\n",
    "        #return model_ckp.tokenizer.prepare_seq2seq_batch(\n",
    "        #    item, max_length=args.max_source_length, return_tensors='pt'\n",
    "        #)\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return self.len    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17e99ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "476\n"
     ]
    }
   ],
   "source": [
    "# Pilot Sample\n",
    "sample_data = TripleLabelDataset(df_sample)\n",
    "sample_dataloader = DataLoader(dataset=sample_data, batch_size=64)\n",
    "print(len(sample_dataloader))\n",
    "\n",
    "# Full Data\n",
    "data = TripleLabelDataset(df)\n",
    "dataloader = DataLoader(dataset=data, batch_size=16)\n",
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5787eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_verbalisation_on_df(row):\n",
    "    try:\n",
    "        replacer = UnknownCharReplacer(model_ckp.tokenizer)\n",
    "        replacer.read_label(row['entity_label'])\n",
    "        replacer.read_label(row['object_label'])\n",
    "        return replacer.replace_on_sentence(row['verbalisation'])\n",
    "    except Exception:\n",
    "        print(row)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87377636",
   "metadata": {},
   "source": [
    "## Pilot Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987cf7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "verbalisations = []\n",
    "start_idx = 0\n",
    "for idx, batch in enumerate(sample_dataloader):\n",
    "    if idx < start_idx:\n",
    "        print(f'Skipping idx {idx}')\n",
    "        continue\n",
    "    print(idx,end=': ')\n",
    "    \n",
    "    inputs_encoding = model_ckp.tokenizer.prepare_seq2seq_batch(\n",
    "        batch, max_length=args.max_source_length, return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    now = time.monotonic()\n",
    "    model_ckp.model.eval()\n",
    "    with torch.no_grad():\n",
    "        gen_output = model_ckp.model.generate(\n",
    "            inputs_encoding['input_ids'],\n",
    "            attention_mask=inputs_encoding['attention_mask'],\n",
    "            use_cache=True,\n",
    "            decoder_start_token_id = model_ckp.decoder_start_token_id,\n",
    "            num_beams=model_ckp.eval_beams,\n",
    "            max_length=model_ckp.eval_max_length,\n",
    "            length_penalty=1.0\n",
    "        )\n",
    "    print('Generated batch in', time.strftime(\"%H:%M:%S\", time.gmtime(time.monotonic() - now)))\n",
    "    \n",
    "    verbalisations = verbalisations + [model_ckp.tokenizer.decode(i) for i in gen_output]\n",
    "  \n",
    "    start_idx += 1\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "86269c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['verbalisation'] = verbalisations\n",
    "df_sample['verbalisation'] = df_sample['verbalisation'].apply(lambda x : x[0].upper() + x[1:])\n",
    "df_sample['processed_verbalisation'] = df_sample.apply(replace_verbalisation_on_df ,axis=1)\n",
    "df_sample.to_csv('pilot_sampled_df_verbalised.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb299b89",
   "metadata": {},
   "source": [
    "## Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c5b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Generated batch in 00:03:15\n",
      "1: Generated batch in 00:05:38\n",
      "2: Generated batch in 00:06:20\n",
      "3: Generated batch in 00:05:50\n",
      "4: Generated batch in 00:05:02\n",
      "5: Generated batch in 00:03:37\n",
      "6: Generated batch in 00:04:07\n",
      "7: Generated batch in 00:05:52\n",
      "8: Generated batch in 00:04:05\n",
      "9: Generated batch in 00:04:51\n",
      "10: Generated batch in 00:06:57\n",
      "11: Generated batch in 00:04:52\n",
      "12: Generated batch in 00:03:45\n",
      "13: Generated batch in 00:04:53\n",
      "14: Generated batch in 00:04:37\n",
      "15: Generated batch in 00:05:03\n",
      "16: Generated batch in 00:03:05\n",
      "17: Generated batch in 00:04:07\n",
      "18: Generated batch in 00:05:04\n",
      "19: Generated batch in 00:04:53\n",
      "20: Generated batch in 00:04:53\n",
      "21: Generated batch in 00:04:08\n",
      "22: Generated batch in 00:03:37\n",
      "23: Generated batch in 00:05:12\n",
      "24: Generated batch in 00:04:17\n",
      "25: Generated batch in 00:04:39\n",
      "26: Generated batch in 00:03:48\n",
      "27: Generated batch in 00:04:27\n",
      "28: Generated batch in 00:03:11\n",
      "29: Generated batch in 00:03:58\n",
      "30: Generated batch in 00:06:03\n",
      "31: Generated batch in 00:06:51\n",
      "32: Generated batch in 00:07:43\n",
      "33: Generated batch in 00:07:21\n",
      "34: Generated batch in 00:06:55\n",
      "35: Generated batch in 00:04:02\n",
      "36: Generated batch in 00:04:09\n",
      "37: Generated batch in 00:04:08\n",
      "38: Generated batch in 00:06:31\n",
      "39: Generated batch in 00:05:58\n",
      "40: Generated batch in 00:04:17\n",
      "41: Generated batch in 00:04:04\n",
      "42: Generated batch in 00:04:45\n",
      "43: Generated batch in 00:04:57\n",
      "44: Generated batch in 00:05:53\n",
      "45: Generated batch in 00:07:19\n",
      "46: Generated batch in 00:10:09\n",
      "47: Generated batch in 00:05:43\n",
      "48: Generated batch in 00:03:43\n",
      "49: Generated batch in 00:08:00\n",
      "50: Generated batch in 00:07:09\n",
      "51: Generated batch in 00:06:30\n",
      "52: Generated batch in 00:05:18\n",
      "53: Generated batch in 00:06:21\n",
      "54: Generated batch in 00:06:11\n",
      "55: Generated batch in 00:09:35\n",
      "56: Generated batch in 00:08:35\n",
      "57: Generated batch in 00:04:06\n",
      "58: Generated batch in 00:06:12\n",
      "59: Generated batch in 00:05:08\n",
      "60: Generated batch in 00:10:33\n",
      "61: Generated batch in 00:09:44\n",
      "62: Generated batch in 00:05:55\n",
      "63: Generated batch in 00:04:41\n",
      "64: Generated batch in 00:07:39\n",
      "65: Generated batch in 00:06:38\n",
      "66: Generated batch in 00:06:54\n",
      "67: Generated batch in 00:05:28\n",
      "68: Generated batch in 00:13:22\n",
      "69: Generated batch in 00:05:38\n",
      "70: Generated batch in 00:04:05\n",
      "71: Generated batch in 00:04:18\n",
      "72: Generated batch in 00:07:12\n",
      "73: Generated batch in 00:06:59\n",
      "74: Generated batch in 00:03:59\n",
      "75: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_idx = 0\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    if idx < start_idx:\n",
    "        print(f'Skipping idx {idx}')\n",
    "        continue\n",
    "    print(idx,end=': ')\n",
    "    \n",
    "    inputs_encoding = model_ckp.tokenizer.prepare_seq2seq_batch(\n",
    "        batch, max_length=args.max_source_length, return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    now = time.monotonic()\n",
    "    model_ckp.model.eval()\n",
    "    with torch.no_grad():\n",
    "        gen_output = model_ckp.model.generate(\n",
    "            inputs_encoding['input_ids'],\n",
    "            attention_mask=inputs_encoding['attention_mask'],\n",
    "            use_cache=True,\n",
    "            decoder_start_token_id = model_ckp.decoder_start_token_id,\n",
    "            num_beams=model_ckp.eval_beams,\n",
    "            max_length=model_ckp.eval_max_length,\n",
    "            length_penalty=1.0\n",
    "        )\n",
    "    print('Generated batch in', time.strftime(\"%H:%M:%S\", time.gmtime(time.monotonic() - now)))\n",
    "    \n",
    "    verbalisations = [model_ckp.tokenizer.decode(i) for i in gen_output]\n",
    "\n",
    "    with open(f'verbalisations/verbalisations_batch_{idx}.txt','w+') as f:\n",
    "        for v in verbalisations:\n",
    "            f.write(v)\n",
    "            f.write('\\n')    \n",
    "    \n",
    "    start_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9f4b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all verbalisations from .txt files\n",
    "import glob\n",
    "n_filenames = len(glob.glob('verbalisations/*.txt'))\n",
    "verbalisations = []\n",
    "\n",
    "for idx in range(n_filenames):\n",
    "    filename = f'verbalisations/verbalisations_batch_{idx}.txt'\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            verbalisations.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a76e2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['verbalisation'] = verbalisations\n",
    "df['verbalisation'] = df['verbalisation'].apply(lambda x : x[0].upper() + x[1:])\n",
    "df['processed_verbalisation'] = df.apply(replace_verbalisation_on_df ,axis=1)\n",
    "df['unk_count'] = df['verbalisation'].apply(lambda x : x.count('⁇'))\n",
    "\n",
    "# Check if verbs are the same for unk = 0\n",
    "df[df['verbalisation'] == df['processed_verbalisation']].equals(df[df['unk_count'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b008eb00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First drop those without labels\n",
    "\n",
    "df = df[(df['entity_label_lan'] != 'none') & (df['property_label_lan'] != 'none') & (df['object_label_lan'] != 'none')].reset_index(drop=True)\n",
    "# Create a new id to stratify per property AND theme\n",
    "df['property_and_theme_id'] = df.apply(lambda x : x['property_id'] + x['theme_entity_id'], axis=1)\n",
    "\n",
    "\n",
    "# Then select those in English\n",
    "df_english = df[(df['entity_label_lan'] == 'en') & (df['property_label_lan'] == 'en') & (df['object_label_lan'] == 'en')].reset_index(drop=True)\n",
    "\n",
    "# Create a group indication column (strata) for splitting (due to fund available we can only analyse a portion)\n",
    "df_english['campaign_group'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d949ac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold as StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=42, shuffle=True, random_state=42)\n",
    "\n",
    "for idx, (train_index, test_index) in enumerate(skf.split(df_english, df_english['theme_entity_id'])):\n",
    "    df_english.loc[test_index, 'campaign_group'] = idx\n",
    "    \n",
    "# Number of examples per group\n",
    "df_english['campaign_group'].value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a568b05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q1248784     9\n",
       "Q2066131     9\n",
       "Q16521       9\n",
       "Q6999        9\n",
       "Q12973014    9\n",
       "Q483501      9\n",
       "Q11173       9\n",
       "Q334166      8\n",
       "Q2095        8\n",
       "Q3918        8\n",
       "Q515         8\n",
       "Q11631       8\n",
       "Q1114461     8\n",
       "Q8502        8\n",
       "Q82955       8\n",
       "Q79007       8\n",
       "Q41176       7\n",
       "Q47461344    7\n",
       "Q4989906     5\n",
       "Q3305213     5\n",
       "Name: theme_entity_id, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of examples per group\n",
    "df_english[df_english['campaign_group'] == 0]['theme_entity_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c82aa7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('campaign_sampled_df_verbalised.csv', index=None)\n",
    "df_english.to_csv('campaign_sampled_df_verbalised_english.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5c4b16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_label</th>\n",
       "      <th>property_label</th>\n",
       "      <th>object_label</th>\n",
       "      <th>verbalisation</th>\n",
       "      <th>processed_verbalisation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Tabubil Airport</td>\n",
       "      <td>located in time zone</td>\n",
       "      <td>UTC+10:00</td>\n",
       "      <td>Tabubil Airport is located in the time zone UT...</td>\n",
       "      <td>Tabubil Airport is located in the time zone UT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Reus Airport</td>\n",
       "      <td>has part</td>\n",
       "      <td>Reial Aero Club de Reus</td>\n",
       "      <td>Reial Aero Club de Reus is part of Reus Airport.</td>\n",
       "      <td>Reial Aero Club de Reus is part of Reus Airport.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Embaba Airport</td>\n",
       "      <td>dissolved, abolished or demolished date</td>\n",
       "      <td>2002</td>\n",
       "      <td>Embaba Airport was dismantled, abolished or de...</td>\n",
       "      <td>Embaba Airport was dismantled, abolished or de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Utti Airport</td>\n",
       "      <td>location</td>\n",
       "      <td>Utti</td>\n",
       "      <td>Utti Airport is located in Utti.</td>\n",
       "      <td>Utti Airport is located in Utti.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Leicester Airport</td>\n",
       "      <td>historic county</td>\n",
       "      <td>Leicestershire</td>\n",
       "      <td>Leicester Airport is located in the historic c...</td>\n",
       "      <td>Leicester Airport is located in the historic c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td>Kodra e Robit</td>\n",
       "      <td>located in time zone</td>\n",
       "      <td>UTC+01:00</td>\n",
       "      <td>Kodra e Robit is located in the time zone UTC+...</td>\n",
       "      <td>Kodra e Robit is located in the time zone UTC+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7378</th>\n",
       "      <td>Hochwang</td>\n",
       "      <td>parent peak</td>\n",
       "      <td>Aroser Rothorn</td>\n",
       "      <td>Aroser Rothorn is the parent peak of Hochwang.</td>\n",
       "      <td>Aroser Rothorn is the parent peak of Hochwang.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7460</th>\n",
       "      <td>Stoppelsberg</td>\n",
       "      <td>part of</td>\n",
       "      <td>Fulda-Haune-Tafelland</td>\n",
       "      <td>Stoppelsberg is part of Fulda-Haune-Tafelland.</td>\n",
       "      <td>Stoppelsberg is part of Fulda-Haune-Tafelland.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7499</th>\n",
       "      <td>Cuhppulčohkka</td>\n",
       "      <td>native label</td>\n",
       "      <td>Cuhppulčohkka</td>\n",
       "      <td>Cuhppul ⁇ ohkka is a native label.</td>\n",
       "      <td>Cuhppulčohkka is a native label.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7552</th>\n",
       "      <td>Turó del Maçaners</td>\n",
       "      <td>continent</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Turó del Maçaners is located on the continent ...</td>\n",
       "      <td>Turó del Maçaners is located on the continent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           entity_label                           property_label  \\\n",
       "28      Tabubil Airport                     located in time zone   \n",
       "58         Reus Airport                                 has part   \n",
       "69       Embaba Airport  dissolved, abolished or demolished date   \n",
       "94         Utti Airport                                 location   \n",
       "142   Leicester Airport                          historic county   \n",
       "...                 ...                                      ...   \n",
       "7368      Kodra e Robit                     located in time zone   \n",
       "7378           Hochwang                              parent peak   \n",
       "7460       Stoppelsberg                                  part of   \n",
       "7499      Cuhppulčohkka                             native label   \n",
       "7552  Turó del Maçaners                                continent   \n",
       "\n",
       "                 object_label  \\\n",
       "28                  UTC+10:00   \n",
       "58    Reial Aero Club de Reus   \n",
       "69                       2002   \n",
       "94                       Utti   \n",
       "142            Leicestershire   \n",
       "...                       ...   \n",
       "7368                UTC+01:00   \n",
       "7378           Aroser Rothorn   \n",
       "7460    Fulda-Haune-Tafelland   \n",
       "7499            Cuhppulčohkka   \n",
       "7552                   Europe   \n",
       "\n",
       "                                          verbalisation  \\\n",
       "28    Tabubil Airport is located in the time zone UT...   \n",
       "58     Reial Aero Club de Reus is part of Reus Airport.   \n",
       "69    Embaba Airport was dismantled, abolished or de...   \n",
       "94                     Utti Airport is located in Utti.   \n",
       "142   Leicester Airport is located in the historic c...   \n",
       "...                                                 ...   \n",
       "7368  Kodra e Robit is located in the time zone UTC+...   \n",
       "7378     Aroser Rothorn is the parent peak of Hochwang.   \n",
       "7460     Stoppelsberg is part of Fulda-Haune-Tafelland.   \n",
       "7499                 Cuhppul ⁇ ohkka is a native label.   \n",
       "7552  Turó del Maçaners is located on the continent ...   \n",
       "\n",
       "                                processed_verbalisation  \n",
       "28    Tabubil Airport is located in the time zone UT...  \n",
       "58     Reial Aero Club de Reus is part of Reus Airport.  \n",
       "69    Embaba Airport was dismantled, abolished or de...  \n",
       "94                     Utti Airport is located in Utti.  \n",
       "142   Leicester Airport is located in the historic c...  \n",
       "...                                                 ...  \n",
       "7368  Kodra e Robit is located in the time zone UTC+...  \n",
       "7378     Aroser Rothorn is the parent peak of Hochwang.  \n",
       "7460     Stoppelsberg is part of Fulda-Haune-Tafelland.  \n",
       "7499                   Cuhppulčohkka is a native label.  \n",
       "7552  Turó del Maçaners is located on the continent ...  \n",
       "\n",
       "[153 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english[df_english['campaign_group'] == 0][['entity_label','property_label','object_label','verbalisation','processed_verbalisation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "f56f7d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_unk_1 = df[df['unk_count'] == 1].copy()\n",
    "#df_unk_1['processed_verbalisation'] = df_unk_1.apply(replace_verbalisation_on_df ,axis=1)\n",
    "#\n",
    "##df_unk_1_bk = df_unk_1.copy()\n",
    "#print(df_unk_1.equals(df_unk_1_bk))\n",
    "##df_unk_1[['entity_label','object_label','processed_verbalisation']].to_csv('test.csv')\n",
    "\n",
    "#df_unk_1[df_unk_1['processed_verbalisation'] != df_unk_1_bk['processed_verbalisation']][['entity_label','object_label','verbalisation','processed_verbalisation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f5391e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13959,     3, 21094,    12,  1566,    10, 32100,  1181,  8020,   235,\n",
      "         32101,  2663,    13, 32102,  4338,  4316,     1]])\n",
      "['Antipasto is an Italian dish.']\n",
      "10.492921352386475\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "inputs = [\n",
    "    '''translate Graph to English: \n",
    "    <H> antipasto <R> aspect of <T> Italian cuisine \n",
    "    ''',\n",
    "]\n",
    "\n",
    "inputs_encoding = model_ckp.tokenizer.prepare_seq2seq_batch(\n",
    "    inputs, max_length=args.max_source_length, return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(inputs_encoding['input_ids'])\n",
    "\n",
    "now = time.time()\n",
    "\n",
    "model_ckp.model.eval()\n",
    "with torch.no_grad():\n",
    "    gen_output = model_ckp.model.generate(\n",
    "        inputs_encoding['input_ids'],\n",
    "        attention_mask=inputs_encoding['attention_mask'],\n",
    "        use_cache=True,\n",
    "        decoder_start_token_id = model_ckp.decoder_start_token_id,\n",
    "        num_beams=model_ckp.eval_beams,\n",
    "        max_length=model_ckp.eval_max_length,\n",
    "        length_penalty=1.0\n",
    "    )\n",
    "\n",
    "print([model_ckp.tokenizer.decode(i) for i in gen_output])\n",
    "print(time.time() - now)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebffa8",
   "metadata": {},
   "source": [
    "## Subject and Object Inverted\n",
    "Using anternate aliases helps\n",
    "'translate Graph to English: <H> 117852 Constance <R> follows <T> (117851) 2005 JE151'\n",
    "'117852 Constance is followed by (117851) 2005 JE151.'\n",
    "'translate Graph to English: <H> 117852 Constance <R> previous is <T> (117851) 2005 JE151'\n",
    "'117852 Constance was preceded by (117851) 2005 JE151.'\n",
    "    \n",
    "'translate Graph to English: <H> Decius <R> child <T> Hostilian'\n",
    "'Decius is a child of Hostilian.'\n",
    "'translate Graph to English: <H> Decius <R>  has child <T> Hostilian'\n",
    "'Decius has a child called Hostilian.'\n",
    "    \n",
    "## Hard Claim Syntax\n",
    "These normally do not have aliases that are any easier to read\n",
    "    \n",
    "## Predicate Meaning Not Understood by Model\n",
    "\n",
    "Artist | Aleksandr Vasilevitsj Vasjakin | conflict | Eastern Front | 1.6\n",
    "Aleksandr Vasilevitsj Vasjakin is in the Eastern Front.\n",
    "conflict -> participated in conflict\n",
    "'Aleksandr Vasilevitsj Vasjakin participated in the conflict at the Eastern Front.'\n",
    "\n",
    "Painting | Fresco depicting a menead carrying a thyrsus | movement | Ancient Roman mural painting | 2.0\n",
    "Fresco depicting a menead carrying a thyrsus is a movement in the Ancient Roman mural painting.\n",
    "artistic movement    \n",
    "Fresco depicting a menead carrying a thyrsus is part of the artistic movement of the Ancient Roman mural painting.\n",
    "    \n",
    "## Redundant Claim Data\n",
    "This is something that is emergent from how Wikidata stores information. For instance, an entity exists for a city, and another for its flag, which includes the flag's image. One is linked to the other by the flag predicate. This makes ontological sense, but no verbal sense, as one would say \"This city has a flag\" or \"This city's flag is this city's flag\", being either redundant or not quite communicating what the claim says. The same is true for things which are specifications/parts of others, like Israel's Cycling team of 1997 is part of Israel's Cycling team.\n",
    "    \n",
    "## Qualifiers needed\n",
    "One would have to find a way of reliably tying qualifiers or descriptors to elements of the claim\n",
    "\n",
    "## Vague predicate\n",
    "Using alternative aliases works. Choosing the proper alias is tricky and depends on the context.\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA 11.2 torch installation",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
